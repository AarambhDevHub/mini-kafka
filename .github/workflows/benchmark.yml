name: 🚀 Mini Kafka Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual runs

jobs:
  benchmark:
    name: 📊 Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - name: 📚 Checkout repository
      uses: actions/checkout@v4
      
    - name: 🦀 Install Rust toolchain
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
        components: rustfmt, clippy
        
    - name: 📦 Cache Cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-bench-
          ${{ runner.os }}-cargo-
        
    - name: 🔧 Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential pkg-config
        
    - name: ⚡ Build project in release mode
      run: cargo build --release
      
    - name: 🧪 Run all tests first
      run: cargo test --release
        
    - name: 📊 Run benchmarks and save results
      run: |
        # Create results directory
        mkdir -p benchmark_results
        
        # Run benchmarks with JSON output for parsing
        cargo bench --message-format=json -- --output-format=json 2>&1 | tee benchmark_results/benchmark_raw.json
        
        # Run benchmarks with human-readable output
        cargo bench 2>&1 | tee benchmark_results/benchmark_readable.txt
        
        # Generate summary report
        echo "# Mini Kafka Benchmark Results" > benchmark_results/README.md
        echo "Generated on: $(date)" >> benchmark_results/README.md
        echo "Commit: ${{ github.sha }}" >> benchmark_results/README.md
        echo "Branch: ${{ github.ref_name }}" >> benchmark_results/README.md
        echo "" >> benchmark_results/README.md
        echo "## Summary" >> benchmark_results/README.md
        tail -20 benchmark_results/benchmark_readable.txt >> benchmark_results/README.md
        
    - name: 📈 Store benchmark result with dashboard
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: Mini Kafka Performance
        tool: 'cargo'
        output-file-path: benchmark_results/benchmark_raw.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # Alert if performance degrades by 150%
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false
        
    - name: 📁 Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mini-kafka-benchmark-results-${{ github.run_number }}
        path: |
          benchmark_results/
          target/criterion/
        retention-days: 90
        
    - name: 📊 Upload Criterion HTML reports
      uses: actions/upload-artifact@v4
      with:
        name: criterion-html-reports-${{ github.run_number }}
        path: target/criterion/
        retention-days: 30
        
    - name: 💾 Create release artifact (on tag)
      if: startsWith(github.ref, 'refs/tags/')
      run: |
        # Create release package with benchmarks
        tar -czf mini-kafka-benchmarks-${{ github.ref_name }}.tar.gz benchmark_results/
        
    - name: 🏷️ Upload to release (on tag)
      if: startsWith(github.ref, 'refs/tags/')
      uses: actions/upload-artifact@v4
      with:
        name: release-benchmarks-${{ github.ref_name }}
        path: mini-kafka-benchmarks-${{ github.ref_name }}.tar.gz
        
    - name: 📱 Notify on failure
      if: failure()
      run: |
        echo "🚨 Benchmark failed! Check the logs above for details."
        echo "This could indicate a performance regression or build issue."
